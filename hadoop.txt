# tips


hive


hive> SHOW DATABASES;


hive> CREATE DATABASE a025660;
OK
Time taken: 0.177 seconds
hive> use a025660
    > ;



hive> create external table sample_sqoop_tbl (ID String, NAme String)  row format delimited fields terminated by ',' location '/user/A025660/hivetest';





sqoop export --connect jdbc:teradata://tddeva/DATABASE=SANDBOX_DEV,LOGMECH=ldap --connection-manager org.apache.sqoop.teradata.TeradataConnManager --table PROMO_POT_SQOOP --export-dir /apps/hive/warehouse/a025660.db/managed_sample_tbl/ --username a025660 --password jan@2015 -m 1 --fields-terminated-by ','



sqoop export --connect jdbc:teradata://tddeva/DATABASE=SANDBOX_DEV,LOGMECH=ldap --connection-manager org.apache.sqoop.teradata.TeradataConnManager --table PROMO_POT_SQOOP --export-dir /apps/hive/warehouse/a025660.db/managed_sample_tbl/ --username a025660 --password jan@2015 -m 1 --fields-terminated-by ','


=======================================================================================================================================================


oozie job -oozie http://d-3zkvk02.target.com:11000/oozie -config job.properties -run



<workflow-app xmlns='uri:oozie:workflow:0.1' name='map-reduce-wf'>
    <start to='sqooptest' />
    <action name='sqooptest'>
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
                    <job-tracker>${jobTracker}</job-tracker>
                    <name-node>${nameNode}</name-node>
<command>sqoop export --connect jdbc:teradata://tddeva/DATABASE=SANDBOX_DEV,LOGMECH=ldap --connection-manager org.apache.sqoop.teradata.TeradataConnManager --table PROMO_POT_SQOOP --export-dir /apps/hive/warehouse/a025660.db/managed_sample_tbl/ --username a025660 --password jan@2015 -m 1 --fields-terminated-by ','</command>
                </sqoop>
        <ok to="end" />
        <error to="fail" />
    </action>
    <kill name="fail">
        <message>Map/Reduce failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name='end' />
</workflow-app>


=========================

#!/bin/bash
sqoop job --delete 'SEM_MODE_RETL_IMPORT' --meta-connect 'jdbc:hsqldb:hsql://d-3zkvk02.target.com:16000/sqoop'
sqoop job --create 'SEM_MODE_RETL_IMPORT' --meta-connect 'jdbc:hsqldb:hsql://d-3zkvk02.target.com:16000/sqoop' -- import --connect 'jdbc:teradata://tdproda/database=DLBIM_PROMO,logmech=LDAP' --table test_SEM_MODE_RETL --hive-import --hive-table a025660.test_SEM_MODE_RETL --username a025660 --password-file /user/A025660/paswd1  --driver=com.teradata.jdbc.TeraDriver --m 1 --fetch-size 10000 --hive-overwrite --hive-drop-import-delims --fields-terminated-by '\001' --lines-terminated-by '\n' --null-string '\\N' --null-non-string '\\N'



sqoop job --exec 'SEM_MODE_RETL_IMPORT'  --meta-connect 'jdbc:hsqldb:hsql://d-3zkvk02.target.com:16000/sqoop'


===================


Hive 0.14.0 (HDP 2.2) - supports insert/update/delete natively.

 set hive.support.concurrency = true;
 set hive.enforce.bucketing = true;
 set hive.exec.dynamic.partition.mode = nonstrict;
 set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
 set hive.compactor.initiator.on = true;
 set hive.compactor.worker.threads = 2;

use a025660;

create table HiveTest
 (EmployeeID Int,FirstName String,Designation String,
 Salary Int,Department String)
 clustered by (department) into 3 buckets
 stored as orc TBLPROPERTIES ('transactional'='true') ;

  insert into table HiveTest values (1,'Gireesh','ABC',1000,'BI'), (3,'Paul','xyc',66000,'PROMOBI1');

select * from hivetest where firstname='Seetharam';

update hivetest set firstname='Seetharam_updated' where employeeID=2;

 delete from hivetest where firstname='Gireesh';


=======




export HADOOP_HOME=/usr/hdp/2.2.0.0-2041/hadoop
export HIVE_HOME=/usr/hdp/2.2.0.0-2041/hive
export HCAT_HOME=/usr/hdp/2.2.0.0-2041/hive-hcatalog/share/hcatalog
export HCAT_LIB_JARS=$HCAT_HOME/hive-hcatalog-core.jar,$HCAT_HOME/hive-hcatalog-server-extensions.jar,$HCAT_HOME/hive-hcatalog-pig-adapter.jar,$HCAT_HOME/hive-hcatalog-streaming.jar,$HIVE_HOME/lib/hive-metastore.jar,$HIVE_HOME/lib/libthrift-0.9.0.jar,$HIVE_HOME/lib/hive-exec.jar,$HIVE_HOME/lib/libfb303-0.9.0.jar,$HIVE_HOME/lib/jdo-api-3.0.1.jar,$HIVE_HOME/lib/hive-cli.jar,$HIVE_HOME/lib/hive-common.jar,$HIVE_HOME/lib/hive-serde.jar

export HADOOP_CLASSPATH=$HCAT_HOME/*:$HIVE_HOME/lib/*:$HIVE_HOME/conf

hadoop jar /usr/hdp/2.2.0.0-2041/sqoop/lib/teradata-connector-1.3.2-hadoop210.jar com.teradata.hadoop.tool.TeradataImportTool --libjars $HCAT_LIB_JARS -url jdbc:teradata://tddeva/DATABASE=SANDBOX_DEV,LOGMECH=ldap -username a025660 -password jan@2015 -jobtype hive -fileformat rcfile -sourcetable example3_td -nummappers 3 -splitbycolumn c1 -targettable example4_hive_tdch

hadoop jar /usr/hdp/2.2.0.0-2041/sqoop/lib/teradata-connector-1.3.2-hadoop210.jar com.teradata.hadoop.tool.TeradataImportTool  --libjars $HCAT_LIB_JARS -url jdbc:teradata://tddeva/DATABASE=TEST19SLS,LOGMECH=ldap -username a025660 -password jan@2015 -jobtype hive -fileformat rcfile -sourcetable RGTN -sourcefieldnames "CO_LOC_I,CRTE_BTCH_I,SLS_D" -nummappers 4 -splitbycolumn SLS_D -targetdatabase a025660 -targettable RGTN_TDCH_CREATED -targettableschema "CO_LOC_I int,CRTE_BTCH_I int" -targetpartitionschema "SLS_D string" -targetfieldnames "CO_LOC_I,CRTE_BTCH_I,SLS_D"


====


desc extended table_name 



===
oozie logs

oozie -info -verbose
http://d-3zjtk02.target.com:8088/proxy/application_1431706812658_130981/

application_1431706812658_130992


========================
